{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [E2E NLG challenge dataset](https://github.com/tuetschek/e2e-dataset) consists of 50k pairs of natural language texts (NLs) and their meaning representations (MRs). For example:\n",
    "\n",
    "- MR:\n",
    "\n",
    "```\n",
    "name[The Eagle],\n",
    "eatType[coffee shop],\n",
    "food[French],\n",
    "priceRange[moderate],\n",
    "customerRating[3/5],\n",
    "area[riverside],\n",
    "kidsFriendly[yes],\n",
    "near[Burger King]\n",
    "```\n",
    "\n",
    "- NL:\n",
    "\n",
    "\n",
    "```\n",
    "The three star coffee shop, The Eagle, gives families a mid-priced dining experience featuring a variety of wines and cheeses. Find The Eagle near Burger King.\n",
    "```\n",
    "\n",
    "This example is taken from the page on the [E2E NLG challenge](http://www.macs.hw.ac.uk/InteractionLab/E2E/). The objective of this challenge is to generate a text given its meaning representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we train a  classifier to label a text with the MR attribute-values. So given the text above as input, the classifier would tell us that it is `eatType[coffee shop]`, `food[French]`, `priceRange[moderate]`, `customerRating[3/5]`, `kidsFriendly[yes]`. `area[riverside]` should not be output as it is not verbalized (the human writers sometimes omitted some information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning a text with its meaning representation in Natural Language Generation: what for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the e2e NLG challenge organizers (see [that paper](https://aclweb.org/anthology/W18-6539)), the NLG model with best results is achieved by [this system](https://aclweb.org/anthology/N18-1014). To improve their results, the authors of that approach use reranking of the NLG outputs by looking at slots in the MR that are missing in the output text (false negative) and slots in the output text that are missing in the input MR (false positive). \n",
    "\n",
    "This is done by a slot aligner that aligns each sentence in the output with a subset of MR slots.  This slot alignment approach uses heuristics based on a gazetteer, a set of hand written rules and access to Wordnet to augment the gazetteer with related terms (e.g., \"italian\" and \"pasta\"). The slot alignment is also used by the authors to generate new data by taking individual sentences and their aligned slots as new input pairs.\n",
    "\n",
    "In this notebook, we align the texts with the MRs, not individual sentences. That is left for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach in this notebook relies on [fastai](https://www.fast.ai/) approach and library for classification using transfer-based NLP and gets an **f-score of 89-90% over all the labels on the test set**.\n",
    "\n",
    "The texts were delexicalized for venue names and this raised the test set f-score (from 85% to 90%) since we don't want labels to depend on names of venues. On the other hand, types of venue `restaurant` have a very low f-score whilst being high in both training and validation set. It is not clear why this happens.\n",
    "\n",
    "Qualitative analysis of texts and MR pairs with most incorrect predictions in the training set reveals 3 main issues with the dataset: some MRs are not verbalized, some texts verbalize different MRs, and some MRs are so close in meaning that they are undistinguishable in the text.\n",
    "\n",
    "Note: This notebook is based on [that github](https://github.com/krasing/multilabel-ULMFiT/blob/master/asrs_new-factors-clean.ipynb) which does multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from fastai import * #Â notebook was run with fastai 1.0.51\n",
    "from fastai.text import *\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# by setting a random seed number, we'll ensure that when doing language model, same training-validation split is used.\n",
    "np.random.seed(42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the e2e NLG challenge dataset, we want to detect content given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/\"trainset.csv\")\n",
    "df_test = pd.read_csv(path/\"testset_w_refs.csv\")\n",
    "df_dev = pd.read_csv(path/\"devset.csv\")\n",
    "print(df.shape)\n",
    "print(df_dev.shape)\n",
    "print(df_test.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove name and near from the features as they are open ended + they are normally a strict match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the name feature-value as it is in every MR and string-based. We replace the name feature value with `near[yes]` to indicate when it is verbalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delexicalize(attribute,value,new_value,new_row,row):\n",
    "    new_row[\"ref\"] = re.sub(value,new_value,new_row[\"ref\"])\n",
    "    new_row[\"ref\"] = re.sub(value.lower(),new_value.lower(),new_row[\"ref\"])\n",
    "    new_row[\"ref\"] = re.sub(strip_accents(value.lower()),new_value.lower(),new_row[\"ref\"])\n",
    "    new_row[\"ref\"] = re.sub(strip_accents(value),new_value,new_row[\"ref\"])\n",
    "    value0=value[0]+value[1:].lower()\n",
    "    new_row[\"ref\"] = re.sub(value0,new_value,new_row[\"ref\"])\n",
    "    new_row[\"ref\"] = re.sub(strip_accents(value0),new_value,new_row[\"ref\"])\n",
    "    value0=value[0].lower()+value[1:]\n",
    "    new_row[\"ref\"] = re.sub(value0,new_value,new_row[\"ref\"])\n",
    "    new_row[\"ref\"] = re.sub(strip_accents(value0),new_value,new_row[\"ref\"])\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "def process_features(df):\n",
    "    rows = []\n",
    "    for i,row in df.iterrows():\n",
    "        row0 = row.to_dict()\n",
    "        row0[\"ref\"] = re.sub(\"  +\",\" \",row0[\"ref\"])\n",
    "        row0[\"mr\"] = re.sub(\"  +\",\" \",row0[\"mr\"])\n",
    "        name = re.sub(r\"^.*name\\[([^\\]]+)\\].*$\",r\"\\1\",row0[\"mr\"].strip())\n",
    "        near = re.sub(r\"^.*near\\[([^\\]]+)\\].*$\",r\"\\1\",row0[\"mr\"].strip())\n",
    "        name = re.sub(\"  +\",\" \",name)\n",
    "        near = re.sub(\"  +\",\" \",near)\n",
    "        row0 = delexicalize(\"name\",name,\"Xxx\",row0,row)\n",
    "        row0 = delexicalize(\"near\",near,\"Yyy\",row0,row)\n",
    "        row0[\"mr\"] = re.sub(r\"name\\[[^\\]]+\\](, *| *$)\",\"\",row0[\"mr\"].strip())\n",
    "        row0[\"mr\"] = re.sub(r\"near\\[[^\\]]+\\](, *| *$)\",r\"near[yes]\\1\",row0[\"mr\"].strip())\n",
    "        row0[\"mr\"] = re.sub(r\", *$\",\"\",row0[\"mr\"].strip())\n",
    "        row0[\"mr\"] = re.sub(r\" *, *\",\",\",row0[\"mr\"].strip())\n",
    "        row0[\"mr\"] = row0[\"mr\"].strip()\n",
    "        if row[\"ref\"]==row0[\"ref\"]:\n",
    "            continue\n",
    "        rows.append(row0)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=process_features(df)\n",
    "df_dev=process_features(df_dev)\n",
    "df_test=process_features(df_test)\n",
    "print(df.shape)\n",
    "print(df_dev.shape)\n",
    "print(df_test.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "rows=[]\n",
    "for i,row in df.iterrows():\n",
    "    mrs = row[\"mr\"].split(\",\")\n",
    "    sents = sent_tokenize(row[\"ref\"])\n",
    "    for mr in mrs:\n",
    "        row[mr]=1\n",
    "        if not mr.startswith(\"near\") and not mr.startswith(\"name\"):\n",
    "            feature_name = re.sub(r\"^([^\\[]+)\\[.*$\",r\"\\1\",mr.strip())\n",
    "            row[feature_name]=1\n",
    "    row[\"num_mrs\"]=len(mrs)\n",
    "    row[\"num_sents\"]=len(sents)\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(rows)\n",
    "df_stats = df_stats.fillna(0)\n",
    "df_stats.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "df_sample = df_stats\n",
    "rows =[]\n",
    "for col in df_sample.columns:\n",
    "    row={}\n",
    "    if df_stats[col].dtype == np.float64:\n",
    "        if \"[\" not in col:\n",
    "            row[\"feature\"]=\"_\"+col\n",
    "        else:\n",
    "            row[\"feature\"]=col\n",
    "        row[\"num\"]=df_sample[col].sum()\n",
    "        row[\"mean\"]=df_sample[col].mean()\n",
    "        row[\"std\"]=df_sample[col].std()\n",
    "        rows.append(row)\n",
    "    elif df_sample[col].dtype == np.int64:\n",
    "        row[\"feature\"]=\"__\"+col\n",
    "        row[\"num_1\"] = (df_sample.loc[df_sample[col]==1]).shape[0]\n",
    "        row[\"num\"]=df_sample[col].sum()\n",
    "        row[\"mean\"]=df_sample[col].mean()\n",
    "        row[\"min\"]=df_sample[col].min()\n",
    "        row[\"max\"]=df_sample[col].max()\n",
    "        row[\"std\"]=df_sample[col].std()\n",
    "        row[\"median\"]=df_sample[col].median()\n",
    "        rows.append(row)\n",
    "df_stats0 = pd.DataFrame(rows)\n",
    "df_stats0 = df_stats0.sort_values(by=\"feature\")\n",
    "df_stats0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine tuning the language model, we use training, validation and test set, as we're not using the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df, df_dev,df_test], ignore_index=True)\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_df(df_all, \".\", cols='ref')\n",
    "                .split_by_rand_pct(0.1)\n",
    "                .label_for_lm()\n",
    "                .databunch(bs=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm = language_model_learner(data_lm, arch=AWD_LSTM, drop_mult=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.lr_find()\n",
    "learn_lm.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.fit_one_cycle(1, 1e-02, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.lr_find()\n",
    "learn_lm.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.fit_one_cycle(4, 1e-03, moms=(0.8,0.7),wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save('fine_tuned')\n",
    "learn_lm.save_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the generative language model to work, giving it some beginning of text for it to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"Near\"\n",
    "N_WORDS = 50\n",
    "N_TEXTS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_TEXTS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(log_preds, targs, thresh=0.5, epsilon=1e-8):\n",
    "    pred_pos = (log_preds > thresh).float()\n",
    "    tpos = torch.mul((targs == pred_pos).float(), targs.float())\n",
    "    return (tpos.sum()/(pred_pos.sum() + epsilon))#.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(log_preds, targs, thresh=0.5, epsilon=1e-8):\n",
    "    pred_pos = (log_preds > thresh).float()\n",
    "    tpos = torch.mul((targs == pred_pos).float(), targs.float())\n",
    "    return (tpos.sum()/(targs.sum() + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = TextClasDataBunch.from_df(\".\", train_df=df, valid_df=df_dev, \n",
    "                                  vocab=data_lm.vocab, \n",
    "                                  text_cols='ref', \n",
    "                                  label_cols='mr',\n",
    "                                  label_delim=',',\n",
    "                                  bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_clas.valid_ds.classes))\n",
    "data_clas.valid_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, arch=AWD_LSTM,drop_mult=1e-7)\n",
    "learn.metrics = [accuracy_thresh, precision, recall]\n",
    "learn.load_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, 1E-02, moms=(0.8,0.7),wd=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, arch=AWD_LSTM,drop_mult=1e-7)\n",
    "learn = learn.load(\"stage1\")\n",
    "learn.metrics = [accuracy_thresh, precision, recall]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we unfreeze the whole model and train some more. I did not find that unfreezing the last 2 layers first made any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, slice(1E-03/(2.6**4),1E-03), moms=(0.8,0.7), wd=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"classifier_model\",return_path=True, with_opt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model_name,df_train,df_valid,vocab,bs):\n",
    "    data_clas = TextClasDataBunch.from_df(\".\", train_df=df_train, valid_df=df_valid, \n",
    "                                      text_cols='ref', \n",
    "                                      label_cols='mr',\n",
    "                                      label_delim=',',\n",
    "                                      bs=bs)\n",
    "    learn = text_classifier_learner(data_clas, arch=AWD_LSTM)\n",
    "    learn.load(model_name)\n",
    "    learn.data = data_clas\n",
    "    preds, y = learn.get_preds(ordered=True)\n",
    "    return learn,preds,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_train,preds_train,y_train = make_predictions(\"classifier_model\",df,df,None,bs)\n",
    "learn_valid,preds_valid,y_valid = make_predictions(\"classifier_model\",df,df_dev,None,bs)\n",
    "learn_valid,preds_test,y_test = make_predictions(\"classifier_model\",df,df_test,None,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train = f1_score(y_train, preds_train>0.5, average='micro')\n",
    "f1_valid = f1_score(y_valid, preds_valid>0.5, average='micro')\n",
    "f1_test = f1_score(y_test, preds_test>0.5, average='micro')\n",
    "f1_train,f1_valid,f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look into more details at each feature performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train = y_train.numpy()\n",
    "scores_train = preds_train.numpy()\n",
    "report = classification_report(y_true_train, scores_train>0.5, target_names=data_clas.valid_ds.classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_valid = y_valid.numpy()\n",
    "scores_valid = preds_valid.numpy()\n",
    "report = classification_report(y_true_valid, scores_valid>0.5, target_names=data_clas.valid_ds.classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = y_test.numpy()\n",
    "scores_test = preds_test.numpy()\n",
    "report = classification_report(y_true_test, scores_test>0.5, target_names=data_clas.valid_ds.classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f-score with test set is near 90%. For some reason the classifier is no good at finding `eatType[restaurant]` mentions, with an f-score of 2%. \n",
    "\n",
    "With the validation set, there are no instances of `eatType[restaurant]` and `eatType[pub]` or `food[Fast food],food[French],food[Indian],food[Italian],food[Japanese]`. \n",
    "\n",
    "With the training set, the f-score is 95% with high f-scores on individual labels.\n",
    "\n",
    "Why is detection of eating venue type so poor in the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge training examples with their predictions in the same dataframe and order rows in ascending order for f-score so we can view the worst predictions first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,preds,y = make_predictions(\"classifier_model\",df,df,None,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, preds>0.5, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_row_metrics(row,true_mrs,predicted_mrs):\n",
    "        tp=0\n",
    "        fp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        for mr in predicted_mrs:\n",
    "            if mr in true_mrs:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        for mr in true_mrs:\n",
    "            if mr not in predicted_mrs:\n",
    "                fn+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        row[\"tp\"]=tp\n",
    "        row[\"fp\"]=fp\n",
    "        row[\"fn\"]=fn\n",
    "        row[\"tn\"]=tn\n",
    "        row[\"precision\"]=0\n",
    "        row[\"recall\"]=0\n",
    "        row[\"fscore\"]=0\n",
    "        if tp+fp>0:\n",
    "            row[\"precision\"]=float(tp)/(tp+fp)\n",
    "        if tp+fn>0:\n",
    "            row[\"recall\"]=float(tp)/(tp+fn)\n",
    "        if row[\"precision\"]+row[\"recall\"]>0:\n",
    "            row[\"fscore\"]= 2*((row[\"precision\"]*row[\"recall\"])/(row[\"precision\"]+row[\"recall\"]))\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(df,preds,classes):\n",
    "    preds_true = (preds>0.5)\n",
    "    counter=0\n",
    "    rows=[]\n",
    "    for i,row in df.iterrows():\n",
    "        row_preds = preds[counter]\n",
    "        indices = [j for j in range(len(preds_true[counter])) if preds_true[counter][j]==True]\n",
    "        row_labels = [classes[j] for j in indices]\n",
    "        row[\"mr_predict\"]=\",\".join(sorted(row_labels))\n",
    "        predicted_mrs = row[\"mr\"].split(\",\")\n",
    "        row[\"mr\"]=\",\".join(sorted(predicted_mrs))\n",
    "        row = set_row_metrics(row,row_labels,predicted_mrs)\n",
    "        rows.append(row)\n",
    "        counter=counter+1\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.valid_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = set_labels(df,preds,learn.data.valid_ds.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that over a third of training instances are a perfect match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[df_preds[\"fscore\"]==1].shape[0]/df_preds.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort dataframe containing real MRs, predicted MRs and corresponding texts, together with fscore and other metrics, in ascending order of fscore, so as to do some error analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = df_preds.sort_values(by=[\"fscore\",\"precision\",\"recall\"],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[df_preds[\"fscore\"]<1][df_preds[\"fscore\"]>0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the texts with low f-score prediction, the following problems appear:\n",
    "\n",
    "1. Some MRs are altogether incorrectly verbalized or sloppily verbalized. For example we have the following true MRs:\n",
    "```\n",
    "area[riverside],customer rating[1 out of 5],food[Fast food],near[yes],priceRange[high]\t\n",
    "```\n",
    "It gets verbalized as:\n",
    "```\n",
    "Alimentum is a one star restaurant near the Yippee Noodle Bar\n",
    "```\n",
    "So `fast food` has been verbalized as `restaurant` which is technically true (we can say \"a fast food restaurant\").\n",
    "\n",
    "2. Some MRs are not verbalized like in the example above where the fact that it is by the riverside is not mentioned.\n",
    "3. There is a problem in that quantiative customer rating and price range like `customerRating[1 to 5]` are sometimes verbalized quantitatively, which is then analysed as `customerRating[low]`. So it seems that one can generate from those MRs but for evaluation, more flexible MRs should be considered: if text says 'low customer rating' then MR can either be `customerRating[low]` or `customerRating[1 to 5]`. For example we have the following true MRs:\n",
    "```\n",
    "eatType[restaurant],familyFriendly[yes],food[Japanese],priceRange[less than Â£20]\n",
    "```\n",
    "It gets verbalized as:\n",
    "```\n",
    "Loch Fyne is a cheap family friendly Japanese restaurant.\n",
    "```\n",
    "which gets classified as:\n",
    "````\n",
    "eatType[restaurant],familyFriendly[yes],food[Japanese],priceRange[cheap]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each alignment, we can mark how many of the original it is missing, for how many it is a mismatch (original says Italian food and alignment says French food), and for how many the alignment added information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(features):\n",
    "    d = {}\n",
    "    features = features.split(\",\")\n",
    "    for f in features:\n",
    "        name = (re.sub(r\"^([^\\[]+)\\[([^\\]]+)\\]$\",r\"\\1\",f)).strip()\n",
    "        value = (re.sub(r\"^([^\\[]+)\\[([^\\]]+)\\]$\",r\"\\2\",f)).strip()\n",
    "        if name not in d.keys():\n",
    "            d[name]=set()\n",
    "        d[name].add(value)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "for i,row in df_preds.iterrows():\n",
    "    row0=row\n",
    "    mrs = convert_to_dict(row[\"mr\"])\n",
    "    mrs_predict = convert_to_dict(row[\"mr_predict\"])\n",
    "    missing=0\n",
    "    mismatch=0\n",
    "    added=0\n",
    "    for feature in mrs.keys():\n",
    "        if feature not in mrs_predict.keys():\n",
    "            missing+=1\n",
    "        else:\n",
    "            for value in mrs[feature]:\n",
    "                if value not in mrs_predict[feature]:\n",
    "                    mismatch+=1\n",
    "                    break\n",
    "    for feature in mrs_predict.keys():\n",
    "        if feature not in mrs.keys():\n",
    "            added+=1\n",
    "    row0[\"missing\"]=missing\n",
    "    row0[\"mismatch\"]=mismatch\n",
    "    row0[\"added\"]=added\n",
    "    rows.append(row0)\n",
    "    \n",
    "pd_preds0 = pd.DataFrame(rows)\n",
    "pd.preds0.sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
